{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifiers\n",
    "\n",
    "#### Jessica Morrise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are willing to assume that all the features in our dataset are independent, we can get a really fast classifier that works extremely well on certain kinds of datasets. Below we code up two examples: Gaussian Naive Bayes and Multinomial Naive Bayes. For both examples, our classifier and sklearn's classifier achieve identical results.\n",
    "\n",
    "## Part 1: Using the Seeds Dataset\n",
    "\n",
    "In this part of the lab, we create a simple Gaussian Naive Bayes classifier to train on the seeds dataset. This dataset contains samples measurements of seeds taken from three different species of wheat. The \"features\" are 7 real-valued measurements: \n",
    "* Area\n",
    "* Perimeter\n",
    "* Compactness\n",
    "* Length\n",
    "* Width\n",
    "* Asymmetry Coefficient\n",
    "* Groove Length\n",
    "\n",
    "The real-valued nature of the features makes this an ideal dataset for training a Gaussian Naive Bayes classifier.\n",
    "\n",
    "Below we load in the data and split it into a training set and a 40-sample test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=[]\n",
    "with open('seeds_dataset.txt','r') as f:\n",
    "    for line in f:\n",
    "        features = map(float, line.strip().split())\n",
    "        data.append(features)\n",
    "        \n",
    "data = np.array(data)\n",
    "N = data.shape[0]\n",
    "train, test = train_test_split(data,test_size=40)\n",
    "train_features = train[:,:7]\n",
    "train_labels = train[:,7]\n",
    "test_features = test[:,:7]\n",
    "test_labels = test[:,7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a Gaussian Naive Bayes classifier, we calculate the mean and variance of each feature for each of the three species. A uniform prior over the species is assumed. To classify a sample, we calculate the log probability of the sample given each of the three species, and assign it to whichever species yields the highest log likelihood. For this particular test and training set, our accuracy is 92.5%. Not bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "37 correctly labeled (92.5 percent accuracy)\n"
     ]
    }
   ],
   "source": [
    "def gaussian_log_prob(features,mus,var):\n",
    "    temp = -0.5*(features-mus)**2/var - np.log((2.*np.pi*var)**0.5)\n",
    "    return np.sum(temp,axis=1)\n",
    "\n",
    "def accuracy(predicted, actual):\n",
    "    n_correct =  np.sum(predicted==actual)\n",
    "    print \"\\n%d correctly labeled (%.1f percent accuracy)\"%(n_correct,100*n_correct/float(np.size(actual)))\n",
    "    \n",
    "variances = []\n",
    "means = []\n",
    "variances.append(np.var(train_features[train_labels==1],axis=0))\n",
    "variances.append(np.var(train_features[train_labels==2],axis=0))\n",
    "variances.append(np.var(train_features[train_labels==3],axis=0))\n",
    "means.append(np.mean(train_features[train_labels==1],axis=0))\n",
    "means.append(np.mean(train_features[train_labels==2],axis=0))\n",
    "means.append(np.mean(train_features[train_labels==3],axis=0))\n",
    "\n",
    "# Do the classification\n",
    "log_probs = np.zeros((test_labels.size,3))\n",
    "for i in xrange(3):\n",
    "    log_probs[:,i] = gaussian_log_prob(test_features,means[i], variances[i])\n",
    "predicted_labels = np.argmax(log_probs,axis=1)+1\n",
    "\n",
    "accuracy(predicted_labels,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training sklearn's GaussianNB classifier on the same training set gives us exactly the same classification accuracy! We basically just hand-coded something from sklearn. Nice work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "37 correctly labeled (92.5 percent accuracy)\n"
     ]
    }
   ],
   "source": [
    "#Now use sklearn\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(train_features,train_labels)\n",
    "predicted_gnb_labels = gnb.predict(test_features)\n",
    "accuracy(predicted_gnb_labels,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Spam Problem\n",
    "\n",
    "Below we implement a class to encapsulate the methods we used above: fit(X,Y) for fitting on training data $X$ and training labels $Y$, and predict(X) for assigning labels to a test set $X$.\n",
    "\n",
    "We will use this classifier to mark emails as \"spam\" or \"not spam\". Our dataset consists of several thousand emails, labeled with a 1 for spam or a 0 for not spam. Each email is stored in a simplified representation as a word count vector. Thus, the Naive Bayes classifier will not be able to use word order to classify emails, only word counts. The classifier will also make the naive assumption that words in an email are independent of each other. As it turns out, this assumption works rather well for spam vs. not spam.\n",
    "\n",
    "Rather than Gaussian probabilities, the spam classifier uses the following:\n",
    "\n",
    "$$p_{ij} = \\frac{count(c_i,v_j) + 1}{\\sum_{j=1}^n(count(c_i,v_j) + 1}$$\n",
    "\n",
    "where $i$ is the index over labels, $j$ is the index over vocabulary words, $count(c_i,v_j)$ denotes the number of occurrences of word $v_j$ among all training documents that have label $c_j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class naiveBayes(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        '''\n",
    "        X: training data\n",
    "           shape is (n_samples, n_features)\n",
    "        Y: training labels\n",
    "           shape is (n_samples)\n",
    "        ''' \n",
    "        self.K = len(set(Y)) # number of unique labels\n",
    "        self.labels = list(set(Y))\n",
    "        N = X.shape[0]\n",
    "        \n",
    "        self.p = np.zeros((self.K,X.shape[1]))\n",
    "        self.prior = np.zeros(self.K)\n",
    "        for k in xrange(self.K):\n",
    "            c_k = self.labels[k]\n",
    "            self.prior[k] = np.sum(Y==c_k)/float(N)\n",
    "            counts = np.sum(X[Y==c_k,:],axis=0)\n",
    "            p_j = (counts+1)/float(np.sum(counts+1))\n",
    "            self.p[k,:] = p_j\n",
    " \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        X: test data\n",
    "           shape is (n_samples, n_features)\n",
    "           \n",
    "        Returns Y: labels of test data\n",
    "        '''\n",
    "        N = X.shape[0]\n",
    "        predicted_labels = np.empty(N)\n",
    "        for j in xrange(N):\n",
    "            log_prob = np.sum(np.log(self.p)*X[j],axis=1) + np.log(self.prior)\n",
    "            k = np.argmax(log_prob)\n",
    "            predicted_labels[j] = self.labels[k]\n",
    "        return predicted_labels   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load in the training data and labels, then separate them into a training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load in spam features\n",
    "spam_features = []\n",
    "with open('SpamFeatures.txt','r') as f:\n",
    "    for line in f:\n",
    "        counts = map(float, line.strip().split())\n",
    "        spam_features.append(counts)\n",
    "feature_matrix = np.array(spam_features)\n",
    "\n",
    "# load in spam labels\n",
    "spam_labels = []\n",
    "with open('SpamLabels.txt','r') as f:\n",
    "    for line in f:\n",
    "        label = int(float(line.strip()))\n",
    "        spam_labels.append(label)\n",
    "label_matrix = np.array(spam_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a training/test set\n",
    "train_spam, test_spam = train_test_split(np.hstack((feature_matrix,np.vstack(label_matrix))), test_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier is surprisingly accurate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "482 correctly labeled (96.4 percent accuracy)\n"
     ]
    }
   ],
   "source": [
    "my_nb = naiveBayes()\n",
    "my_nb.fit(train_spam[:,:-1], train_spam[:,-1])\n",
    "predicted_spam_labels = my_nb.predict(test_spam[:,:-1])\n",
    "accuracy(predicted_spam_labels, test_spam[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet again, sklearn's implementation yields precisely the same accuracy. Why do we even have sklearn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "482 correctly labeled (96.4 percent accuracy)\n"
     ]
    }
   ],
   "source": [
    "multi_nb = MultinomialNB()\n",
    "multi_nb.fit(train_spam[:,:-1], train_spam[:,-1])\n",
    "predicted_mnb_spam_labels = multi_nb.predict(test_spam[:,:-1])\n",
    "accuracy(predicted_mnb_spam_labels,test_spam[:,-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
